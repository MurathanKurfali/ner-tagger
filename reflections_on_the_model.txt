I fine-tuned and compared three transformer models (BERT, RoBERTa, and XLNet). All models performed similarly well in both configurations.

In both system configurations, tag-wise performances were remarkably consistent.
Contrary to my expectations of better performance on less frequent tags in System B (due to the model having fewer concepts to learn),
this was not the case. This indicates that having fine-grained tags does not negatively impact overall or tag-specific performance.

The models notably excelled in the most common tags, which initially made me suspicious. Suspecting a potential data overlap between
training and test data in terms of entities,  I assessed the models only on the unseen <token, tag> pairs of the test set. Yet, their
performance remained stable. This highlights the models' robust generalization capabilities.

However, this generalization may be specific to the domain at hand because the baseline model in the original paper exhibited
significantly poorer performance on other datasets, such as OntoNotes, hinting at a limitation in the adaptability of
the models trained on this dataset. Moreover, the model still struggles with less common tags, which I attempted to
address by employing weighted cross-entropy, but with no success. An additional limitation is the lack of any hyperparameter search.