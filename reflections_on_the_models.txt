I fine-tuned and compared three transformer models (BERT, RoBERTa, and XLNet), which demonstrated nearly equal performances in both configurations.

In both system configurations, tag-wise performances were remarkably consistent. Contrary to my expectations, the performance was not
better on less frequent tags in System B, despite the model having fewer concepts to learn. This indicates that having fine-grained tags
does not negatively impact overall or tag-specific performance.

The models notably excelled in the most common tags, which initially made me suspicious. Suspecting a potential data overlap between training
and test data in terms of entities, I assessed the models only on the unseen <token, tag> pairs of the test set. Yet, their performances
remained stable. This highlights the models' robust generalization capabilities.

However, this generalization may be specific to the domain at hand since the baseline model in the original paper exhibited significantly
poorer performance on other datasets, e.g. OntoNotes, hinting at a limitation in the adaptability of the models trained on this dataset.
Moreover, the model still struggles with less common tags, which I attempted to address by employing weighted cross-entropy, yet this
did not improve the results. An additional limitation is the lack of any hyperparameter search.